{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd345bec-88e2-44c6-94be-e78efa266aa1",
   "metadata": {},
   "source": [
    "# **Natuarl Language Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5454d-7de7-4694-a802-4a1a9525cc23",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) and Linguistics that focuses on enabling machines to understand, interpret, generate, and respond to human language â€” just like a human would.\n",
    "\n",
    "Example: Imagine you're talking to Alexa or ChatGPT.\n",
    "\n",
    "\n",
    "### Advantages \n",
    "* Automates tasks like summarizing documents or sorting emails\n",
    "* Improves customer experience (chatbots, recommendations)\n",
    "* Enables insights from massive unstructured data\n",
    "\n",
    "### Real-World Applications\n",
    "| Industry     | Use Case                                    |\n",
    "| ------------ | ------------------------------------------- |\n",
    "| E-commerce   | Product reviews sentiment analysis          |\n",
    "| Healthcare   | Extracting symptoms from clinical notes     |\n",
    "| Banking      | Chatbots for customer service               |\n",
    "| Social Media | Hate speech and spam detection              |\n",
    "| Legal/HR     | Resume screening and document summarization |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb94083-2b6c-4c9d-aea4-ddebe6928b19",
   "metadata": {},
   "source": [
    "## Basic Term used in NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786baea-2258-48fd-9537-1b4ddeb80d92",
   "metadata": {},
   "source": [
    "1. **Corpus:** A corpus is a collection of entir text documents or say a paragraph.\n",
    "```python\n",
    "corpus = {\n",
    "    \"Hi, your OTP is 1234\",\n",
    "    \"Win $10,000 now! Click here\",\n",
    "    \"You have a meeting at 3:00 PM today\"\n",
    "}\n",
    "print(corpus)\n",
    "```\n",
    "\n",
    "Alternative example of corpus\n",
    "\n",
    "```python\n",
    "corpus = {\n",
    "    \"\"\"\n",
    "    Once upon a time, a farmer had a goose that laid a golden egg every day. The farmer used to sell that egg and earn enough money to meet their family's day-to-day needs. One day, the farmer thought that if he could get more such golden eggs and make a lot of money and become a wealthy person.\n",
    "    \"\"\"\n",
    "}\n",
    "print(corpus)\n",
    "```\n",
    "\n",
    "2. **Document:** A document is a single text entry or unit within a corpus.\n",
    "``` python \n",
    "document = \"Win â‚¹10,000 now! Click here\"\n",
    "print(document)\n",
    "```\n",
    "\n",
    "3. **Vocabulary:** The vocabulary is the set of unique words or tokens found across the entire corpus.\n",
    "```python\n",
    "text = [\"I love NLP\", \"NLP is fun\", \"I love Python\"]\n",
    "vocabulary = {\"I\", \"love\", \"NLP\", \"is\", \"fun\", \"Python\"}\n",
    "```\n",
    "\n",
    "4. **Words / Tokens:** Words are the basic units of language (e.g., \"dog\", \"run\", \"beautiful\"). Tokens are the result of breaking text into smaller pieces (usually words, sometimes sub-words or characters) â€” the process is called tokenization. \n",
    "\n",
    "```python\n",
    "text = \"I love NLP!\"\n",
    "tokenization =  [\"I\", \"love\", \"NLP\", \"!\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10553714-181d-4c96-a98a-0c7e1787b687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mksmu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mksmu\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Documents in Corpus:\n",
      "Document 1: Natural Language Processing is fascinating.\n",
      "Document 2: I love exploring machine learning and NLP.\n",
      "Document 3: NLP includes text classification, translation, and generation.\n",
      "\n",
      "ðŸ”¤ Tokens per Document:\n",
      "Document 1 Tokens: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n",
      "Document 2 Tokens: ['I', 'love', 'exploring', 'machine', 'learning', 'and', 'NLP', '.']\n",
      "Document 3 Tokens: ['NLP', 'includes', 'text', 'classification', ',', 'translation', ',', 'and', 'generation', '.']\n",
      "\n",
      "ðŸ“š Vocabulary:\n",
      "['and', 'classification', 'exploring', 'fascinating', 'generation', 'i', 'includes', 'is', 'language', 'learning', 'love', 'machine', 'natural', 'nlp', 'processing', 'text', 'translation']\n",
      "\n",
      "ðŸ§® Vocabulary Size: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "## Initial Example \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "## Download required tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Step 1: Define the corpus (a collection of documents)\n",
    "corpus = [\n",
    "    \"Natural Language Processing is fascinating.\",\n",
    "    \"I love exploring machine learning and NLP.\",\n",
    "    \"NLP includes text classification, translation, and generation.\"\n",
    "]\n",
    "\n",
    "# Step 2: Each item in the corpus is a document\n",
    "print(\"ðŸ“„ Documents in Corpus:\")\n",
    "for i, doc in enumerate(corpus, start=1):\n",
    "    print(f\"Document {i}: {doc}\")\n",
    "\n",
    "# Step 3: Tokenize each document into words (tokens)\n",
    "tokenized_docs = []\n",
    "for doc in corpus:\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokenized_docs.append(tokens)\n",
    "\n",
    "# Step 4: Show words/tokens\n",
    "print(\"\\nðŸ”¤ Tokens per Document:\")\n",
    "for i, tokens in enumerate(tokenized_docs, start=1):\n",
    "    print(f\"Document {i} Tokens: {tokens}\")\n",
    "\n",
    "# Step 5: Build the Vocabulary (set of unique tokens across all documents)\n",
    "# Removing punctuation and lowercasing\n",
    "vocab = set()\n",
    "for tokens in tokenized_docs:\n",
    "    for token in tokens:\n",
    "        if token not in string.punctuation:\n",
    "            vocab.add(token.lower())\n",
    "\n",
    "print(\"\\nðŸ“š Vocabulary:\")\n",
    "print(sorted(vocab))\n",
    "\n",
    "print(f\"\\nðŸ§® Vocabulary Size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ba561-bd20-4df4-a2aa-705bb6e8fbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
